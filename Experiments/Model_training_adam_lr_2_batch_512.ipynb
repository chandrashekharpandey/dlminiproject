{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "42a236e7-d372-4dba-b263-2b9232004c34",
      "metadata": {
        "id": "42a236e7-d372-4dba-b263-2b9232004c34",
        "outputId": "528a07a5-ac2d-4816-b8ed-f5f9fa259bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.8/site-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.22.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.8/site-packages (from torchvision) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
      "metadata": {
        "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
        "outputId": "be1d516e-2c8c-4823-b079-0bf20218a812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch-summary in ./.local/lib/python3.8/site-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3259d544-50b2-43bc-8aa4-e00adde97302",
      "metadata": {
        "id": "3259d544-50b2-43bc-8aa4-e00adde97302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ddf8ae-a49f-428b-d5e3-6b35637a329f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from torch.optim.optimizer import Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35",
      "metadata": {
        "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35"
      },
      "outputs": [],
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b",
      "metadata": {
        "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b"
      },
      "outputs": [],
      "source": [
        "class Lookahead(Optimizer):\n",
        "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
        "\n",
        "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
        "        \"\"\"optimizer: inner optimizer\n",
        "        la_steps (int): number of lookahead steps\n",
        "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
        "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self._la_step = 0  # counter for inner optimizer\n",
        "        self.la_alpha = la_alpha\n",
        "        self._total_la_steps = la_steps\n",
        "        pullback_momentum = pullback_momentum.lower()\n",
        "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
        "        self.pullback_momentum = pullback_momentum\n",
        "\n",
        "        self.state = defaultdict(dict)\n",
        "\n",
        "        # Cache the current optimizer parameters\n",
        "        for group in optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
        "                param_state['cached_params'].copy_(p.data)\n",
        "                if self.pullback_momentum == \"pullback\":\n",
        "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'state': self.state,\n",
        "            'optimizer': self.optimizer,\n",
        "            'la_alpha': self.la_alpha,\n",
        "            '_la_step': self._la_step,\n",
        "            '_total_la_steps': self._total_la_steps,\n",
        "            'pullback_momentum': self.pullback_momentum\n",
        "        }\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_la_step(self):\n",
        "        return self._la_step\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def _backup_and_load_cache(self):\n",
        "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
        "        \"\"\"\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
        "                param_state['backup_params'].copy_(p.data)\n",
        "                p.data.copy_(param_state['cached_params'])\n",
        "\n",
        "    def _clear_and_load_backup(self):\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                p.data.copy_(param_state['backup_params'])\n",
        "                del param_state['backup_params']\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single Lookahead optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = self.optimizer.step(closure)\n",
        "        self._la_step += 1\n",
        "\n",
        "        if self._la_step >= self._total_la_steps:\n",
        "            self._la_step = 0\n",
        "            # Lookahead and cache the current optimizer parameters\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    param_state = self.state[p]\n",
        "                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
        "                    param_state['cached_params'].copy_(p.data)\n",
        "                    if self.pullback_momentum == \"pullback\":\n",
        "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
        "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
        "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                    elif self.pullback_momentum == \"reset\":\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18",
      "metadata": {
        "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  \n",
        "start_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
      "metadata": {
        "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106dc0bf-32e2-4353-965d-d0b5bc8f199a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=kernel_size, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, block, num_blocks=[2,2,2,2], num_classes=10, kernel_size=3, pool_size=4):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, kernel_size=kernel_size)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, kernel_size=kernel_size)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, kernel_size=kernel_size)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, kernel_size=kernel_size)\n",
        "        self.avgpool = nn.AvgPool2d(pool_size)\n",
        "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride, kernel_size):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride, kernel_size=kernel_size))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ULsnxt4xRp_z"
      },
      "id": "ULsnxt4xRp_z",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
      "metadata": {
        "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff8780c8-bb92-4b00-b2da-1bdb32abfbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Conv2d: 1-1                            192\n",
            "├─BatchNorm2d: 1-2                       128\n",
            "├─Sequential: 1-3                        --\n",
            "|    └─BasicBlock: 2-1                   --\n",
            "|    |    └─Conv2d: 3-1                  36,864\n",
            "|    |    └─BatchNorm2d: 3-2             128\n",
            "|    |    └─Conv2d: 3-3                  36,864\n",
            "|    |    └─BatchNorm2d: 3-4             128\n",
            "|    |    └─Sequential: 3-5              --\n",
            "├─Sequential: 1-4                        --\n",
            "|    └─BasicBlock: 2-2                   --\n",
            "|    |    └─Conv2d: 3-6                  73,728\n",
            "|    |    └─BatchNorm2d: 3-7             256\n",
            "|    |    └─Conv2d: 3-8                  147,456\n",
            "|    |    └─BatchNorm2d: 3-9             256\n",
            "|    |    └─Sequential: 3-10             8,448\n",
            "├─Sequential: 1-5                        --\n",
            "|    └─BasicBlock: 2-3                   --\n",
            "|    |    └─Conv2d: 3-11                 294,912\n",
            "|    |    └─BatchNorm2d: 3-12            512\n",
            "|    |    └─Conv2d: 3-13                 589,824\n",
            "|    |    └─BatchNorm2d: 3-14            512\n",
            "|    |    └─Sequential: 3-15             33,280\n",
            "├─Sequential: 1-6                        --\n",
            "|    └─BasicBlock: 2-4                   --\n",
            "|    |    └─Conv2d: 3-16                 1,179,648\n",
            "|    |    └─BatchNorm2d: 3-17            1,024\n",
            "|    |    └─Conv2d: 3-18                 2,359,296\n",
            "|    |    └─BatchNorm2d: 3-19            1,024\n",
            "|    |    └─Sequential: 3-20             132,096\n",
            "├─AvgPool2d: 1-7                         --\n",
            "├─Linear: 1-8                            5,130\n",
            "=================================================================\n",
            "Total params: 4,901,706\n",
            "Trainable params: 4,901,706\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Conv2d: 1-1                            192\n",
            "├─BatchNorm2d: 1-2                       128\n",
            "├─Sequential: 1-3                        --\n",
            "|    └─BasicBlock: 2-1                   --\n",
            "|    |    └─Conv2d: 3-1                  36,864\n",
            "|    |    └─BatchNorm2d: 3-2             128\n",
            "|    |    └─Conv2d: 3-3                  36,864\n",
            "|    |    └─BatchNorm2d: 3-4             128\n",
            "|    |    └─Sequential: 3-5              --\n",
            "├─Sequential: 1-4                        --\n",
            "|    └─BasicBlock: 2-2                   --\n",
            "|    |    └─Conv2d: 3-6                  73,728\n",
            "|    |    └─BatchNorm2d: 3-7             256\n",
            "|    |    └─Conv2d: 3-8                  147,456\n",
            "|    |    └─BatchNorm2d: 3-9             256\n",
            "|    |    └─Sequential: 3-10             8,448\n",
            "├─Sequential: 1-5                        --\n",
            "|    └─BasicBlock: 2-3                   --\n",
            "|    |    └─Conv2d: 3-11                 294,912\n",
            "|    |    └─BatchNorm2d: 3-12            512\n",
            "|    |    └─Conv2d: 3-13                 589,824\n",
            "|    |    └─BatchNorm2d: 3-14            512\n",
            "|    |    └─Sequential: 3-15             33,280\n",
            "├─Sequential: 1-6                        --\n",
            "|    └─BasicBlock: 2-4                   --\n",
            "|    |    └─Conv2d: 3-16                 1,179,648\n",
            "|    |    └─BatchNorm2d: 3-17            1,024\n",
            "|    |    └─Conv2d: 3-18                 2,359,296\n",
            "|    |    └─BatchNorm2d: 3-19            1,024\n",
            "|    |    └─Sequential: 3-20             132,096\n",
            "├─AvgPool2d: 1-7                         --\n",
            "├─Linear: 1-8                            5,130\n",
            "=================================================================\n",
            "Total params: 4,901,706\n",
            "Trainable params: 4,901,706\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration :\n",
        "\n",
        "\n",
        "model = ResNet18(BasicBlock,  num_blocks = [1,1,1,1],kernel_size=1,pool_size=4)\n",
        "\n",
        "print(summary(model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  \n",
        "start_epoch = 0"
      ],
      "metadata": {
        "id": "Bjl4S0ySr_C8"
      },
      "id": "Bjl4S0ySr_C8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
      "metadata": {
        "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d829da14-2123-4a64-eb70-729c980e4967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 0\n",
            "Train Loss: 1.330 | Train Acc: 37.154% (18577/50000) | time: 21.761 seconds\n",
            "Test Loss: 1.100 | Test Acc: 49.790% (4979/10000)  | time: 1.175 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "Train Loss: 0.980 | Train Acc: 54.928% (27464/50000) | time: 19.959 seconds\n",
            "Test Loss: 0.868 | Test Acc: 62.300% (6230/10000)  | time: 1.211 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "Train Loss: 0.787 | Train Acc: 64.244% (32122/50000) | time: 20.175 seconds\n",
            "Test Loss: 0.760 | Test Acc: 66.430% (6643/10000)  | time: 1.087 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            "Train Loss: 0.644 | Train Acc: 71.030% (35515/50000) | time: 19.981 seconds\n",
            "Test Loss: 0.659 | Test Acc: 71.970% (7197/10000)  | time: 1.090 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            "Train Loss: 0.542 | Train Acc: 75.726% (37863/50000) | time: 20.388 seconds\n",
            "Test Loss: 0.470 | Test Acc: 80.130% (8013/10000)  | time: 1.145 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 5\n",
            "Train Loss: 0.479 | Train Acc: 78.644% (39322/50000) | time: 21.119 seconds\n",
            "Test Loss: 0.446 | Test Acc: 81.340% (8134/10000)  | time: 1.240 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            "Train Loss: 0.431 | Train Acc: 80.798% (40399/50000) | time: 22.253 seconds\n",
            "Test Loss: 0.413 | Test Acc: 82.270% (8227/10000)  | time: 1.210 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            "Train Loss: 0.387 | Train Acc: 82.840% (41420/50000) | time: 20.668 seconds\n",
            "Test Loss: 0.439 | Test Acc: 82.190% (8219/10000)  | time: 1.084 seconds\n",
            "\n",
            "Epoch: 8\n",
            "Train Loss: 0.363 | Train Acc: 83.956% (41978/50000) | time: 20.825 seconds\n",
            "Test Loss: 0.487 | Test Acc: 79.670% (7967/10000)  | time: 1.188 seconds\n",
            "\n",
            "Epoch: 9\n",
            "Train Loss: 0.333 | Train Acc: 85.048% (42524/50000) | time: 20.274 seconds\n",
            "Test Loss: 0.344 | Test Acc: 85.250% (8525/10000)  | time: 1.178 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 10\n",
            "Train Loss: 0.309 | Train Acc: 86.144% (43072/50000) | time: 20.729 seconds\n",
            "Test Loss: 0.333 | Test Acc: 86.310% (8631/10000)  | time: 1.079 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 11\n",
            "Train Loss: 0.291 | Train Acc: 87.204% (43602/50000) | time: 19.346 seconds\n",
            "Test Loss: 0.321 | Test Acc: 86.630% (8663/10000)  | time: 1.188 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 12\n",
            "Train Loss: 0.270 | Train Acc: 87.970% (43985/50000) | time: 20.614 seconds\n",
            "Test Loss: 0.347 | Test Acc: 85.900% (8590/10000)  | time: 1.071 seconds\n",
            "\n",
            "Epoch: 13\n",
            "Train Loss: 0.258 | Train Acc: 88.394% (44197/50000) | time: 20.595 seconds\n",
            "Test Loss: 0.381 | Test Acc: 85.180% (8518/10000)  | time: 1.212 seconds\n",
            "\n",
            "Epoch: 14\n",
            "Train Loss: 0.242 | Train Acc: 89.274% (44637/50000) | time: 20.079 seconds\n",
            "Test Loss: 0.313 | Test Acc: 87.290% (8729/10000)  | time: 1.113 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 15\n",
            "Train Loss: 0.229 | Train Acc: 89.788% (44894/50000) | time: 21.420 seconds\n",
            "Test Loss: 0.325 | Test Acc: 86.960% (8696/10000)  | time: 1.131 seconds\n",
            "\n",
            "Epoch: 16\n",
            "Train Loss: 0.219 | Train Acc: 90.138% (45069/50000) | time: 20.344 seconds\n",
            "Test Loss: 0.324 | Test Acc: 87.470% (8747/10000)  | time: 1.107 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 17\n",
            "Train Loss: 0.204 | Train Acc: 90.752% (45376/50000) | time: 19.964 seconds\n",
            "Test Loss: 0.310 | Test Acc: 87.800% (8780/10000)  | time: 1.081 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 18\n",
            "Train Loss: 0.196 | Train Acc: 91.116% (45558/50000) | time: 19.627 seconds\n",
            "Test Loss: 0.337 | Test Acc: 86.870% (8687/10000)  | time: 1.172 seconds\n",
            "\n",
            "Epoch: 19\n",
            "Train Loss: 0.186 | Train Acc: 91.698% (45849/50000) | time: 21.123 seconds\n",
            "Test Loss: 0.274 | Test Acc: 89.320% (8932/10000)  | time: 1.190 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            "Train Loss: 0.175 | Train Acc: 92.076% (46038/50000) | time: 19.691 seconds\n",
            "Test Loss: 0.300 | Test Acc: 88.490% (8849/10000)  | time: 1.101 seconds\n",
            "\n",
            "Epoch: 21\n",
            "Train Loss: 0.169 | Train Acc: 92.474% (46237/50000) | time: 19.932 seconds\n",
            "Test Loss: 0.286 | Test Acc: 89.550% (8955/10000)  | time: 1.179 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 22\n",
            "Train Loss: 0.160 | Train Acc: 92.838% (46419/50000) | time: 19.822 seconds\n",
            "Test Loss: 0.360 | Test Acc: 87.210% (8721/10000)  | time: 1.051 seconds\n",
            "\n",
            "Epoch: 23\n",
            "Train Loss: 0.153 | Train Acc: 93.072% (46536/50000) | time: 20.044 seconds\n",
            "Test Loss: 0.346 | Test Acc: 88.380% (8838/10000)  | time: 1.155 seconds\n",
            "\n",
            "Epoch: 24\n",
            "Train Loss: 0.146 | Train Acc: 93.510% (46755/50000) | time: 19.979 seconds\n",
            "Test Loss: 0.286 | Test Acc: 89.670% (8967/10000)  | time: 1.180 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 25\n",
            "Train Loss: 0.137 | Train Acc: 93.700% (46850/50000) | time: 20.930 seconds\n",
            "Test Loss: 0.297 | Test Acc: 89.600% (8960/10000)  | time: 1.189 seconds\n",
            "\n",
            "Epoch: 26\n",
            "Train Loss: 0.135 | Train Acc: 93.848% (46924/50000) | time: 21.219 seconds\n",
            "Test Loss: 0.316 | Test Acc: 89.000% (8900/10000)  | time: 1.144 seconds\n",
            "\n",
            "Epoch: 27\n",
            "Train Loss: 0.128 | Train Acc: 94.344% (47172/50000) | time: 20.257 seconds\n",
            "Test Loss: 0.331 | Test Acc: 88.670% (8867/10000)  | time: 1.157 seconds\n",
            "\n",
            "Epoch: 28\n",
            "Train Loss: 0.127 | Train Acc: 94.246% (47123/50000) | time: 20.302 seconds\n",
            "Test Loss: 0.317 | Test Acc: 89.230% (8923/10000)  | time: 1.072 seconds\n",
            "\n",
            "Epoch: 29\n",
            "Train Loss: 0.119 | Train Acc: 94.616% (47308/50000) | time: 21.357 seconds\n",
            "Test Loss: 0.299 | Test Acc: 89.510% (8951/10000)  | time: 2.048 seconds\n",
            "\n",
            "Epoch: 30\n",
            "Train Loss: 0.112 | Train Acc: 95.028% (47514/50000) | time: 29.042 seconds\n",
            "Test Loss: 0.294 | Test Acc: 89.750% (8975/10000)  | time: 2.041 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 31\n",
            "Train Loss: 0.113 | Train Acc: 94.842% (47421/50000) | time: 28.666 seconds\n",
            "Test Loss: 0.337 | Test Acc: 89.260% (8926/10000)  | time: 2.018 seconds\n",
            "\n",
            "Epoch: 32\n",
            "Train Loss: 0.109 | Train Acc: 95.062% (47531/50000) | time: 28.713 seconds\n",
            "Test Loss: 0.330 | Test Acc: 89.220% (8922/10000)  | time: 2.017 seconds\n",
            "\n",
            "Epoch: 33\n",
            "Train Loss: 0.104 | Train Acc: 95.238% (47619/50000) | time: 28.581 seconds\n",
            "Test Loss: 0.337 | Test Acc: 88.750% (8875/10000)  | time: 2.029 seconds\n",
            "\n",
            "Epoch: 34\n",
            "Train Loss: 0.099 | Train Acc: 95.452% (47726/50000) | time: 28.780 seconds\n",
            "Test Loss: 0.308 | Test Acc: 89.790% (8979/10000)  | time: 2.006 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 35\n",
            "Train Loss: 0.092 | Train Acc: 95.828% (47914/50000) | time: 28.636 seconds\n",
            "Test Loss: 0.321 | Test Acc: 90.030% (9003/10000)  | time: 2.025 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 36\n",
            "Train Loss: 0.094 | Train Acc: 95.836% (47918/50000) | time: 28.653 seconds\n",
            "Test Loss: 0.314 | Test Acc: 90.330% (9033/10000)  | time: 1.702 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 37\n",
            "Train Loss: 0.090 | Train Acc: 96.000% (48000/50000) | time: 28.789 seconds\n",
            "Test Loss: 0.329 | Test Acc: 89.660% (8966/10000)  | time: 1.365 seconds\n",
            "\n",
            "Epoch: 38\n",
            "Train Loss: 0.085 | Train Acc: 96.180% (48090/50000) | time: 28.571 seconds\n",
            "Test Loss: 0.334 | Test Acc: 89.850% (8985/10000)  | time: 2.020 seconds\n",
            "\n",
            "Epoch: 39\n",
            "Train Loss: 0.084 | Train Acc: 96.234% (48117/50000) | time: 28.720 seconds\n",
            "Test Loss: 0.312 | Test Acc: 90.470% (9047/10000)  | time: 2.024 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 40\n",
            "Train Loss: 0.081 | Train Acc: 96.404% (48202/50000) | time: 28.708 seconds\n",
            "Test Loss: 0.314 | Test Acc: 90.260% (9026/10000)  | time: 2.018 seconds\n",
            "\n",
            "Epoch: 41\n",
            "Train Loss: 0.078 | Train Acc: 96.414% (48207/50000) | time: 28.670 seconds\n",
            "Test Loss: 0.348 | Test Acc: 90.040% (9004/10000)  | time: 2.032 seconds\n",
            "\n",
            "Epoch: 42\n",
            "Train Loss: 0.079 | Train Acc: 96.450% (48225/50000) | time: 28.709 seconds\n",
            "Test Loss: 0.345 | Test Acc: 90.050% (9005/10000)  | time: 2.015 seconds\n",
            "\n",
            "Epoch: 43\n",
            "Train Loss: 0.078 | Train Acc: 96.526% (48263/50000) | time: 28.669 seconds\n",
            "Test Loss: 0.368 | Test Acc: 89.890% (8989/10000)  | time: 2.056 seconds\n",
            "\n",
            "Epoch: 44\n",
            "Train Loss: 0.073 | Train Acc: 96.694% (48347/50000) | time: 28.774 seconds\n",
            "Test Loss: 0.328 | Test Acc: 90.280% (9028/10000)  | time: 2.030 seconds\n",
            "\n",
            "Epoch: 45\n",
            "Train Loss: 0.074 | Train Acc: 96.722% (48361/50000) | time: 28.776 seconds\n",
            "Test Loss: 0.324 | Test Acc: 90.310% (9031/10000)  | time: 2.013 seconds\n",
            "\n",
            "Epoch: 46\n",
            "Train Loss: 0.072 | Train Acc: 96.714% (48357/50000) | time: 28.711 seconds\n",
            "Test Loss: 0.355 | Test Acc: 89.440% (8944/10000)  | time: 2.047 seconds\n",
            "\n",
            "Epoch: 47\n",
            "Train Loss: 0.066 | Train Acc: 97.026% (48513/50000) | time: 28.667 seconds\n",
            "Test Loss: 0.334 | Test Acc: 90.510% (9051/10000)  | time: 1.642 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 48\n",
            "Train Loss: 0.067 | Train Acc: 96.996% (48498/50000) | time: 28.769 seconds\n",
            "Test Loss: 0.353 | Test Acc: 90.060% (9006/10000)  | time: 1.746 seconds\n",
            "\n",
            "Epoch: 49\n",
            "Train Loss: 0.065 | Train Acc: 97.034% (48517/50000) | time: 28.725 seconds\n",
            "Test Loss: 0.338 | Test Acc: 91.200% (9120/10000)  | time: 2.019 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 50\n",
            "Train Loss: 0.063 | Train Acc: 97.136% (48568/50000) | time: 28.712 seconds\n",
            "Test Loss: 0.324 | Test Acc: 91.110% (9111/10000)  | time: 2.031 seconds\n",
            "Saving..\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "import time\n",
        "def train(epoch):\n",
        "    start_time = time.time()\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    end_time = time.time()\n",
        "    acc = 100.*correct/total\n",
        "    loss = 100.*train_loss/total\n",
        "    print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d) | time: %.3f seconds'\n",
        "                     % (loss, acc, correct, total, end_time-start_time))\n",
        "    model_results[str(epoch)] =  {\"train\" : {\"acc\" : acc,\"loss\" : loss},\"test\" : {}}\n",
        "    \n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    loss = 100.*test_loss/total\n",
        "    print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)  | time: %.3f seconds'\n",
        "                     % (loss, acc, correct, total,end_time-start_time))\n",
        "    model_results[str(epoch)]['test']  = {\"acc\" : acc,\"loss\" : loss}\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint_adam'):\n",
        "            os.mkdir('checkpoint_adam')\n",
        "        torch.save(state, './checkpoint_adam/ckpt_256_lr_2_512.pth')\n",
        "        best_acc = acc\n",
        "        \n",
        "#Model Parameters\n",
        "\n",
        "batch_size = 512\n",
        "lr = 0.01\n",
        "optim_param = {'la_steps':5,\n",
        "               'la_alpha':0.5\n",
        "              }\n",
        "resume = False \n",
        "model_results = {}\n",
        "\n",
        "#Load model\n",
        "        \n",
        "net = model\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "\n",
        "if resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint_adam'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint_adam/ckpt_256_lr_2_512.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    Cutout(n_holes=1, length=8)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "base_optim = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "Q = math.floor(len(trainset)/batch_size)\n",
        "optimizer = Lookahead(base_optim, **optim_param)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Q)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+epochs+1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77",
      "metadata": {
        "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('result_adam_256_lr_2_512.json', 'w') as fp:\n",
        "    json.dump(model_results, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45",
      "metadata": {
        "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('./checkpoint_adam/ckpt_256_lr_2_512.pth')\n",
        "net.load_state_dict(checkpoint['net'])\n",
        "best_acc = checkpoint['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
      "metadata": {
        "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10b72a7-b52f-43e3-855a-efdf81cdf693"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.20"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ff3aabef-3499-45f1-ab91-574369acb0c0",
      "metadata": {
        "id": "ff3aabef-3499-45f1-ab91-574369acb0c0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}