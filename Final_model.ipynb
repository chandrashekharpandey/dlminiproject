{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "42a236e7-d372-4dba-b263-2b9232004c34",
      "metadata": {
        "id": "42a236e7-d372-4dba-b263-2b9232004c34",
        "outputId": "006e06a4-cccf-47f7-f42a-0a8ddd7d5e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.8/site-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in ./.local/lib/python3.8/site-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.22.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.8/site-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from torchvision) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
      "metadata": {
        "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
        "outputId": "2cc9b2bd-5ec6-472a-eb9e-ad43b4524d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch-summary in ./.local/lib/python3.8/site-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3259d544-50b2-43bc-8aa4-e00adde97302",
      "metadata": {
        "id": "3259d544-50b2-43bc-8aa4-e00adde97302"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from torch.optim.optimizer import Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35",
      "metadata": {
        "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35"
      },
      "outputs": [],
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b",
      "metadata": {
        "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b"
      },
      "outputs": [],
      "source": [
        "class Lookahead(Optimizer):\n",
        "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
        "\n",
        "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
        "        \"\"\"optimizer: inner optimizer\n",
        "        la_steps (int): number of lookahead steps\n",
        "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
        "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self._la_step = 0  # counter for inner optimizer\n",
        "        self.la_alpha = la_alpha\n",
        "        self._total_la_steps = la_steps\n",
        "        pullback_momentum = pullback_momentum.lower()\n",
        "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
        "        self.pullback_momentum = pullback_momentum\n",
        "\n",
        "        self.state = defaultdict(dict)\n",
        "\n",
        "        # Cache the current optimizer parameters\n",
        "        for group in optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
        "                param_state['cached_params'].copy_(p.data)\n",
        "                if self.pullback_momentum == \"pullback\":\n",
        "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'state': self.state,\n",
        "            'optimizer': self.optimizer,\n",
        "            'la_alpha': self.la_alpha,\n",
        "            '_la_step': self._la_step,\n",
        "            '_total_la_steps': self._total_la_steps,\n",
        "            'pullback_momentum': self.pullback_momentum\n",
        "        }\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_la_step(self):\n",
        "        return self._la_step\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.optimizer.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.optimizer.load_state_dict(state_dict)\n",
        "\n",
        "    def _backup_and_load_cache(self):\n",
        "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
        "        \"\"\"\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
        "                param_state['backup_params'].copy_(p.data)\n",
        "                p.data.copy_(param_state['cached_params'])\n",
        "\n",
        "    def _clear_and_load_backup(self):\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for p in group['params']:\n",
        "                param_state = self.state[p]\n",
        "                p.data.copy_(param_state['backup_params'])\n",
        "                del param_state['backup_params']\n",
        "\n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single Lookahead optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = self.optimizer.step(closure)\n",
        "        self._la_step += 1\n",
        "\n",
        "        if self._la_step >= self._total_la_steps:\n",
        "            self._la_step = 0\n",
        "            # Lookahead and cache the current optimizer parameters\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    param_state = self.state[p]\n",
        "                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
        "                    param_state['cached_params'].copy_(p.data)\n",
        "                    if self.pullback_momentum == \"pullback\":\n",
        "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
        "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
        "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
        "                    elif self.pullback_momentum == \"reset\":\n",
        "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18",
      "metadata": {
        "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  \n",
        "start_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
      "metadata": {
        "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e0f844-42d7-4b5c-8fbe-ff86eb7fa979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=kernel_size, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, block, num_blocks=[2,2,2,2], num_classes=10, kernel_size=3, pool_size=4):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, kernel_size=kernel_size)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, kernel_size=kernel_size)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, kernel_size=kernel_size)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, kernel_size=kernel_size)\n",
        "        self.avgpool = nn.AvgPool2d(pool_size)\n",
        "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride, kernel_size):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride, kernel_size=kernel_size))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ULsnxt4xRp_z"
      },
      "id": "ULsnxt4xRp_z",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
      "metadata": {
        "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de0da29-a991-48c4-a4e1-579f6348881e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Conv2d: 1-1                            192\n",
            "├─BatchNorm2d: 1-2                       128\n",
            "├─Sequential: 1-3                        --\n",
            "|    └─BasicBlock: 2-1                   --\n",
            "|    |    └─Conv2d: 3-1                  36,864\n",
            "|    |    └─BatchNorm2d: 3-2             128\n",
            "|    |    └─Conv2d: 3-3                  36,864\n",
            "|    |    └─BatchNorm2d: 3-4             128\n",
            "|    |    └─Sequential: 3-5              --\n",
            "├─Sequential: 1-4                        --\n",
            "|    └─BasicBlock: 2-2                   --\n",
            "|    |    └─Conv2d: 3-6                  73,728\n",
            "|    |    └─BatchNorm2d: 3-7             256\n",
            "|    |    └─Conv2d: 3-8                  147,456\n",
            "|    |    └─BatchNorm2d: 3-9             256\n",
            "|    |    └─Sequential: 3-10             8,448\n",
            "├─Sequential: 1-5                        --\n",
            "|    └─BasicBlock: 2-3                   --\n",
            "|    |    └─Conv2d: 3-11                 294,912\n",
            "|    |    └─BatchNorm2d: 3-12            512\n",
            "|    |    └─Conv2d: 3-13                 589,824\n",
            "|    |    └─BatchNorm2d: 3-14            512\n",
            "|    |    └─Sequential: 3-15             33,280\n",
            "├─Sequential: 1-6                        --\n",
            "|    └─BasicBlock: 2-4                   --\n",
            "|    |    └─Conv2d: 3-16                 1,179,648\n",
            "|    |    └─BatchNorm2d: 3-17            1,024\n",
            "|    |    └─Conv2d: 3-18                 2,359,296\n",
            "|    |    └─BatchNorm2d: 3-19            1,024\n",
            "|    |    └─Sequential: 3-20             132,096\n",
            "├─AvgPool2d: 1-7                         --\n",
            "├─Linear: 1-8                            5,130\n",
            "=================================================================\n",
            "Total params: 4,901,706\n",
            "Trainable params: 4,901,706\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Conv2d: 1-1                            192\n",
            "├─BatchNorm2d: 1-2                       128\n",
            "├─Sequential: 1-3                        --\n",
            "|    └─BasicBlock: 2-1                   --\n",
            "|    |    └─Conv2d: 3-1                  36,864\n",
            "|    |    └─BatchNorm2d: 3-2             128\n",
            "|    |    └─Conv2d: 3-3                  36,864\n",
            "|    |    └─BatchNorm2d: 3-4             128\n",
            "|    |    └─Sequential: 3-5              --\n",
            "├─Sequential: 1-4                        --\n",
            "|    └─BasicBlock: 2-2                   --\n",
            "|    |    └─Conv2d: 3-6                  73,728\n",
            "|    |    └─BatchNorm2d: 3-7             256\n",
            "|    |    └─Conv2d: 3-8                  147,456\n",
            "|    |    └─BatchNorm2d: 3-9             256\n",
            "|    |    └─Sequential: 3-10             8,448\n",
            "├─Sequential: 1-5                        --\n",
            "|    └─BasicBlock: 2-3                   --\n",
            "|    |    └─Conv2d: 3-11                 294,912\n",
            "|    |    └─BatchNorm2d: 3-12            512\n",
            "|    |    └─Conv2d: 3-13                 589,824\n",
            "|    |    └─BatchNorm2d: 3-14            512\n",
            "|    |    └─Sequential: 3-15             33,280\n",
            "├─Sequential: 1-6                        --\n",
            "|    └─BasicBlock: 2-4                   --\n",
            "|    |    └─Conv2d: 3-16                 1,179,648\n",
            "|    |    └─BatchNorm2d: 3-17            1,024\n",
            "|    |    └─Conv2d: 3-18                 2,359,296\n",
            "|    |    └─BatchNorm2d: 3-19            1,024\n",
            "|    |    └─Sequential: 3-20             132,096\n",
            "├─AvgPool2d: 1-7                         --\n",
            "├─Linear: 1-8                            5,130\n",
            "=================================================================\n",
            "Total params: 4,901,706\n",
            "Trainable params: 4,901,706\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration :\n",
        "\n",
        "\n",
        "model = ResNet18(BasicBlock,  num_blocks = [1,1,1,1],kernel_size=1,pool_size=4)\n",
        "\n",
        "print(summary(model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  \n",
        "start_epoch = 0"
      ],
      "metadata": {
        "id": "Bjl4S0ySr_C8"
      },
      "id": "Bjl4S0ySr_C8",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef0W2ifnorLX",
        "outputId": "b7093afd-ccc3-49d9-acff-a99353a7c2e0"
      },
      "id": "ef0W2ifnorLX",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
      "metadata": {
        "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4a138a-3fc0-467b-bf63-fd21960f6167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 0\n",
            "Train Loss: 1.357 | Train Acc: 35.964% (17982/50000) | time: 19.586 seconds\n",
            "Test Loss: 1.136 | Test Acc: 48.630% (4863/10000)  | time: 1.123 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "Train Loss: 0.963 | Train Acc: 55.568% (27784/50000) | time: 19.586 seconds\n",
            "Test Loss: 0.835 | Test Acc: 62.840% (6284/10000)  | time: 1.102 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "Train Loss: 0.754 | Train Acc: 65.906% (32953/50000) | time: 19.962 seconds\n",
            "Test Loss: 0.829 | Test Acc: 65.500% (6550/10000)  | time: 1.157 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            "Train Loss: 0.616 | Train Acc: 72.206% (36103/50000) | time: 19.614 seconds\n",
            "Test Loss: 0.582 | Test Acc: 74.850% (7485/10000)  | time: 1.184 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            "Train Loss: 0.530 | Train Acc: 76.158% (38079/50000) | time: 19.464 seconds\n",
            "Test Loss: 0.470 | Test Acc: 79.770% (7977/10000)  | time: 1.071 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 5\n",
            "Train Loss: 0.468 | Train Acc: 79.112% (39556/50000) | time: 19.754 seconds\n",
            "Test Loss: 0.450 | Test Acc: 80.970% (8097/10000)  | time: 1.078 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            "Train Loss: 0.423 | Train Acc: 81.360% (40680/50000) | time: 19.649 seconds\n",
            "Test Loss: 0.430 | Test Acc: 82.150% (8215/10000)  | time: 1.129 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            "Train Loss: 0.385 | Train Acc: 82.944% (41472/50000) | time: 19.519 seconds\n",
            "Test Loss: 0.407 | Test Acc: 82.830% (8283/10000)  | time: 1.172 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 8\n",
            "Train Loss: 0.352 | Train Acc: 84.536% (42268/50000) | time: 19.689 seconds\n",
            "Test Loss: 0.439 | Test Acc: 82.390% (8239/10000)  | time: 1.107 seconds\n",
            "\n",
            "Epoch: 9\n",
            "Train Loss: 0.326 | Train Acc: 85.476% (42738/50000) | time: 19.762 seconds\n",
            "Test Loss: 0.345 | Test Acc: 85.990% (8599/10000)  | time: 1.177 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 10\n",
            "Train Loss: 0.306 | Train Acc: 86.322% (43161/50000) | time: 19.612 seconds\n",
            "Test Loss: 0.334 | Test Acc: 86.420% (8642/10000)  | time: 1.216 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 11\n",
            "Train Loss: 0.284 | Train Acc: 87.280% (43640/50000) | time: 19.462 seconds\n",
            "Test Loss: 0.353 | Test Acc: 85.490% (8549/10000)  | time: 1.066 seconds\n",
            "\n",
            "Epoch: 12\n",
            "Train Loss: 0.265 | Train Acc: 88.106% (44053/50000) | time: 22.687 seconds\n",
            "Test Loss: 0.382 | Test Acc: 85.160% (8516/10000)  | time: 1.084 seconds\n",
            "\n",
            "Epoch: 13\n",
            "Train Loss: 0.252 | Train Acc: 88.828% (44414/50000) | time: 20.511 seconds\n",
            "Test Loss: 0.327 | Test Acc: 86.830% (8683/10000)  | time: 1.184 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            "Train Loss: 0.240 | Train Acc: 89.338% (44669/50000) | time: 19.613 seconds\n",
            "Test Loss: 0.299 | Test Acc: 87.600% (8760/10000)  | time: 1.174 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 15\n",
            "Train Loss: 0.228 | Train Acc: 89.848% (44924/50000) | time: 19.636 seconds\n",
            "Test Loss: 0.309 | Test Acc: 87.490% (8749/10000)  | time: 1.181 seconds\n",
            "\n",
            "Epoch: 16\n",
            "Train Loss: 0.215 | Train Acc: 90.510% (45255/50000) | time: 19.393 seconds\n",
            "Test Loss: 0.312 | Test Acc: 88.100% (8810/10000)  | time: 1.080 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 17\n",
            "Train Loss: 0.201 | Train Acc: 91.172% (45586/50000) | time: 20.550 seconds\n",
            "Test Loss: 0.321 | Test Acc: 87.930% (8793/10000)  | time: 1.147 seconds\n",
            "\n",
            "Epoch: 18\n",
            "Train Loss: 0.193 | Train Acc: 91.322% (45661/50000) | time: 19.631 seconds\n",
            "Test Loss: 0.362 | Test Acc: 86.270% (8627/10000)  | time: 1.096 seconds\n",
            "\n",
            "Epoch: 19\n",
            "Train Loss: 0.179 | Train Acc: 91.946% (45973/50000) | time: 19.550 seconds\n",
            "Test Loss: 0.301 | Test Acc: 88.740% (8874/10000)  | time: 1.101 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            "Train Loss: 0.173 | Train Acc: 92.234% (46117/50000) | time: 19.688 seconds\n",
            "Test Loss: 0.279 | Test Acc: 89.040% (8904/10000)  | time: 1.131 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 21\n",
            "Train Loss: 0.163 | Train Acc: 92.696% (46348/50000) | time: 19.366 seconds\n",
            "Test Loss: 0.299 | Test Acc: 88.900% (8890/10000)  | time: 1.190 seconds\n",
            "\n",
            "Epoch: 22\n",
            "Train Loss: 0.160 | Train Acc: 92.892% (46446/50000) | time: 19.537 seconds\n",
            "Test Loss: 0.296 | Test Acc: 89.210% (8921/10000)  | time: 1.121 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 23\n",
            "Train Loss: 0.151 | Train Acc: 93.094% (46547/50000) | time: 19.873 seconds\n",
            "Test Loss: 0.366 | Test Acc: 86.840% (8684/10000)  | time: 1.148 seconds\n",
            "\n",
            "Epoch: 24\n",
            "Train Loss: 0.144 | Train Acc: 93.544% (46772/50000) | time: 19.524 seconds\n",
            "Test Loss: 0.279 | Test Acc: 89.930% (8993/10000)  | time: 1.117 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 25\n",
            "Train Loss: 0.135 | Train Acc: 93.890% (46945/50000) | time: 20.472 seconds\n",
            "Test Loss: 0.314 | Test Acc: 89.370% (8937/10000)  | time: 1.110 seconds\n",
            "\n",
            "Epoch: 26\n",
            "Train Loss: 0.134 | Train Acc: 93.970% (46985/50000) | time: 19.716 seconds\n",
            "Test Loss: 0.279 | Test Acc: 89.840% (8984/10000)  | time: 1.106 seconds\n",
            "\n",
            "Epoch: 27\n",
            "Train Loss: 0.127 | Train Acc: 94.228% (47114/50000) | time: 19.512 seconds\n",
            "Test Loss: 0.317 | Test Acc: 89.440% (8944/10000)  | time: 1.102 seconds\n",
            "\n",
            "Epoch: 28\n",
            "Train Loss: 0.122 | Train Acc: 94.458% (47229/50000) | time: 19.564 seconds\n",
            "Test Loss: 0.380 | Test Acc: 87.910% (8791/10000)  | time: 1.067 seconds\n",
            "\n",
            "Epoch: 29\n",
            "Train Loss: 0.116 | Train Acc: 94.660% (47330/50000) | time: 19.637 seconds\n",
            "Test Loss: 0.307 | Test Acc: 89.810% (8981/10000)  | time: 1.065 seconds\n",
            "\n",
            "Epoch: 30\n",
            "Train Loss: 0.117 | Train Acc: 94.712% (47356/50000) | time: 19.577 seconds\n",
            "Test Loss: 0.286 | Test Acc: 90.370% (9037/10000)  | time: 1.067 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 31\n",
            "Train Loss: 0.108 | Train Acc: 95.184% (47592/50000) | time: 20.244 seconds\n",
            "Test Loss: 0.345 | Test Acc: 88.850% (8885/10000)  | time: 1.214 seconds\n",
            "\n",
            "Epoch: 32\n",
            "Train Loss: 0.106 | Train Acc: 95.248% (47624/50000) | time: 20.170 seconds\n",
            "Test Loss: 0.297 | Test Acc: 90.130% (9013/10000)  | time: 1.106 seconds\n",
            "\n",
            "Epoch: 33\n",
            "Train Loss: 0.101 | Train Acc: 95.460% (47730/50000) | time: 19.895 seconds\n",
            "Test Loss: 0.323 | Test Acc: 89.340% (8934/10000)  | time: 1.194 seconds\n",
            "\n",
            "Epoch: 34\n",
            "Train Loss: 0.098 | Train Acc: 95.554% (47777/50000) | time: 19.747 seconds\n",
            "Test Loss: 0.302 | Test Acc: 90.730% (9073/10000)  | time: 1.141 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 35\n",
            "Train Loss: 0.098 | Train Acc: 95.732% (47866/50000) | time: 19.612 seconds\n",
            "Test Loss: 0.285 | Test Acc: 90.590% (9059/10000)  | time: 1.108 seconds\n",
            "\n",
            "Epoch: 36\n",
            "Train Loss: 0.091 | Train Acc: 95.886% (47943/50000) | time: 19.789 seconds\n",
            "Test Loss: 0.326 | Test Acc: 90.200% (9020/10000)  | time: 1.078 seconds\n",
            "\n",
            "Epoch: 37\n",
            "Train Loss: 0.087 | Train Acc: 96.048% (48024/50000) | time: 19.447 seconds\n",
            "Test Loss: 0.381 | Test Acc: 89.040% (8904/10000)  | time: 1.095 seconds\n",
            "\n",
            "Epoch: 38\n",
            "Train Loss: 0.086 | Train Acc: 96.152% (48076/50000) | time: 19.659 seconds\n",
            "Test Loss: 0.366 | Test Acc: 89.240% (8924/10000)  | time: 1.092 seconds\n",
            "\n",
            "Epoch: 39\n",
            "Train Loss: 0.086 | Train Acc: 96.144% (48072/50000) | time: 19.829 seconds\n",
            "Test Loss: 0.319 | Test Acc: 90.250% (9025/10000)  | time: 1.094 seconds\n",
            "\n",
            "Epoch: 40\n",
            "Train Loss: 0.082 | Train Acc: 96.314% (48157/50000) | time: 20.128 seconds\n",
            "Test Loss: 0.309 | Test Acc: 90.450% (9045/10000)  | time: 1.078 seconds\n",
            "\n",
            "Epoch: 41\n",
            "Train Loss: 0.080 | Train Acc: 96.490% (48245/50000) | time: 19.560 seconds\n",
            "Test Loss: 0.314 | Test Acc: 90.760% (9076/10000)  | time: 1.079 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 42\n",
            "Train Loss: 0.077 | Train Acc: 96.560% (48280/50000) | time: 19.959 seconds\n",
            "Test Loss: 0.328 | Test Acc: 90.240% (9024/10000)  | time: 1.104 seconds\n",
            "\n",
            "Epoch: 43\n",
            "Train Loss: 0.074 | Train Acc: 96.690% (48345/50000) | time: 19.635 seconds\n",
            "Test Loss: 0.314 | Test Acc: 90.760% (9076/10000)  | time: 1.074 seconds\n",
            "\n",
            "Epoch: 44\n",
            "Train Loss: 0.068 | Train Acc: 96.934% (48467/50000) | time: 21.731 seconds\n",
            "Test Loss: 0.305 | Test Acc: 90.990% (9099/10000)  | time: 1.077 seconds\n",
            "Saving..\n",
            "\n",
            "Epoch: 45\n",
            "Train Loss: 0.074 | Train Acc: 96.646% (48323/50000) | time: 19.729 seconds\n",
            "Test Loss: 0.346 | Test Acc: 89.970% (8997/10000)  | time: 1.112 seconds\n",
            "\n",
            "Epoch: 46\n",
            "Train Loss: 0.072 | Train Acc: 96.786% (48393/50000) | time: 19.203 seconds\n",
            "Test Loss: 0.354 | Test Acc: 90.080% (9008/10000)  | time: 1.065 seconds\n",
            "\n",
            "Epoch: 47\n",
            "Train Loss: 0.067 | Train Acc: 97.020% (48510/50000) | time: 19.548 seconds\n",
            "Test Loss: 0.337 | Test Acc: 91.400% (9140/10000)  | time: 1.148 seconds\n",
            "\n",
            "Epoch: 48\n",
            "Train Loss: 0.065 | Train Acc: 97.132% (48566/50000) | time: 20.595 seconds\n",
            "Test Loss: 0.384 | Test Acc: 90.070% (9007/10000)  | time: 1.097 seconds\n",
            "\n",
            "Epoch: 49\n",
            "Train Loss: 0.065 | Train Acc: 97.100% (48550/50000) | time: 19.473 seconds\n",
            "Test Loss: 0.334 | Test Acc: 90.740% (9074/10000)  | time: 1.087 seconds\n",
            "\n",
            "Epoch: 50\n",
            "Train Loss: 0.062 | Train Acc: 97.224% (48612/50000) | time: 20.437 seconds\n",
            "Test Loss: 0.326 | Test Acc: 91.360% (9136/10000)  | time: 1.139 seconds\n",
            "Saving..\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "import time\n",
        "def train(epoch):\n",
        "    start_time = time.time()\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    end_time = time.time()\n",
        "    acc = 100.*correct/total\n",
        "    loss = 100.*train_loss/total\n",
        "    print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d) | time: %.3f seconds'\n",
        "                     % (loss, acc, correct, total, end_time-start_time))\n",
        "    model_results[str(epoch)] =  {\"train\" : {\"acc\" : acc,\"loss\" : loss},\"test\" : {}}\n",
        "    \n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    loss = 100.*test_loss/total\n",
        "    print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)  | time: %.3f seconds'\n",
        "                     % (loss, acc, correct, total,end_time-start_time))\n",
        "    model_results[str(epoch)]['test']  = {\"acc\" : acc,\"loss\" : loss}\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint_adam'):\n",
        "            os.mkdir('checkpoint_adam')\n",
        "        torch.save(state, './checkpoint_adam/ckpt_256_lr_2_final.pth')\n",
        "        best_acc = acc\n",
        "        \n",
        "#Model Parameters\n",
        "\n",
        "batch_size = 256\n",
        "lr = 1e-2\n",
        "optim_param = {'la_steps':5,\n",
        "               'la_alpha':0.5\n",
        "              }\n",
        "resume = False \n",
        "model_results = {}\n",
        "\n",
        "#Load model\n",
        "        \n",
        "net = model\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "\n",
        "if resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint_adam'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint_adam/ckpt_256_lr_2_final.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    Cutout(n_holes=1, length=8)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "base_optim = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "Q = math.floor(len(trainset)/batch_size)\n",
        "optimizer = Lookahead(base_optim, **optim_param)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Q)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+epochs+1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77",
      "metadata": {
        "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('result_adam_256_lr_2_final.json', 'w') as fp:\n",
        "    json.dump(model_results, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45",
      "metadata": {
        "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('./checkpoint_adam/ckpt_256_lr_2_final.pth')\n",
        "net.load_state_dict(checkpoint['net'])\n",
        "best_acc = checkpoint['acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
      "metadata": {
        "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3055cde-688c-4be4-81ac-8a6ed973ab33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.4"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ff3aabef-3499-45f1-ab91-574369acb0c0",
      "metadata": {
        "id": "ff3aabef-3499-45f1-ab91-574369acb0c0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}